{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4606c43-10f4-4285-b68f-003b92766f5f",
   "metadata": {},
   "source": [
    "# Music Genre Classification using Audio Features\n",
    "\n",
    "### Uses these libraries `torch`, `pandas`, `scipy`, `evaluate`, `numpy`, `sklearn` and `datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfc753-7bcd-46d1-9a01-cadbb988f7e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797c0fe-61b6-4f09-a643-241805b14abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ac656-c76f-4255-a902-00fb116f9e9b",
   "metadata": {},
   "source": [
    "### 2. Load the GTZAN dataset from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6107695f-afd7-486e-b20b-553fdc5cea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934791aa7a6b4f35b93894006597bd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"marsyas/gtzan\", \"audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ade868-8b38-47d8-90ca-b374ae554c0b",
   "metadata": {},
   "source": [
    "#### Get audio features using librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de4ca9d-5875-4abf-bc94-e9ecb0399573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_array, sample_rate):\n",
    "    # Import librosa here to avoid conflicts\n",
    "    import librosa\n",
    "    \n",
    "    # Extract various audio features\n",
    "    # Mel-frequency cepstral coefficients\n",
    "    mfccs = librosa.feature.mfcc(y=audio_array, sr=sample_rate, n_mfcc=13)\n",
    "    \n",
    "    # Spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio_array, sr=sample_rate)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_array, sr=sample_rate)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_array, sr=sample_rate)[0]\n",
    "    \n",
    "    # Rhythm features\n",
    "    tempo, _ = librosa.beat.beat_track(y=audio_array, sr=sample_rate)\n",
    "    \n",
    "    # Zero crossing rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_array)[0]\n",
    "    \n",
    "    # Compute statistics for each feature\n",
    "    features = {}\n",
    "    \n",
    "    # MFCC stats\n",
    "    for i in range(mfccs.shape[0]):\n",
    "        features[f'mfcc{i+1}_mean'] = np.mean(mfccs[i])\n",
    "        features[f'mfcc{i+1}_std'] = np.std(mfccs[i])\n",
    "        features[f'mfcc{i+1}_skew'] = stats.skew(mfccs[i])\n",
    "        features[f'mfcc{i+1}_kurtosis'] = stats.kurtosis(mfccs[i])\n",
    "    \n",
    "    # Other features stats\n",
    "    for name, feature in [\n",
    "        ('spectral_centroid', spectral_centroid),\n",
    "        ('spectral_bandwidth', spectral_bandwidth), \n",
    "        ('spectral_rolloff', spectral_rolloff),\n",
    "        ('zero_crossing_rate', zero_crossing_rate)\n",
    "    ]:\n",
    "        features[f'{name}_mean'] = np.mean(feature)\n",
    "        features[f'{name}_std'] = np.std(feature)\n",
    "        features[f'{name}_skew'] = stats.skew(feature)\n",
    "        features[f'{name}_kurtosis'] = stats.kurtosis(feature)\n",
    "    \n",
    "    # Add tempo\n",
    "    features['tempo'] = tempo\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121c0b0-4b86-4f48-98b6-ee896434742c",
   "metadata": {},
   "source": [
    "#### Process audio files and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a18ad677-5517-4a0e-8519-ea8f0ef61e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_split):\n",
    "    features_list = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in dataset_split:\n",
    "        audio = item['audio']\n",
    "        audio_array = audio['array']\n",
    "        sample_rate = audio['sampling_rate']\n",
    "        genre = item['genre']\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(audio_array, sample_rate)\n",
    "        features_list.append(features)\n",
    "        labels.append(genre)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(features_list)\n",
    "    \n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74751f-a62c-4d12-b2ce-42fae2168ffc",
   "metadata": {},
   "source": [
    "#### Process a smaller subset for quick demonstration (adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddab7e84-e518-433b-bd36-17a9892544be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio features (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting audio features (this may take a while)...\")\n",
    "train_size = 300  # Adjust based on your computational resources\n",
    "test_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6e462-62bd-4841-9188-cd2b396a1ac0",
   "metadata": {},
   "source": [
    "#### Randomly sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884c397e-9b33-4b50-b337-57bffc7159bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m train_indices = np.random.choice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])), train_size, replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_indices = np.random.choice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m])), test_size, replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m train_subset = [dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices]\n\u001b[32m      5\u001b[39m test_subset = [dataset[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/COM424E/lib/python3.13/site-packages/datasets/dataset_dict.py:72\u001b[39m, in \u001b[36mDatasetDict.__getitem__\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) -> Dataset:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getitem__\u001b[39m(k)\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m         available_suggested_splits = [\n\u001b[32m     75\u001b[39m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split.TRAIN, Split.TEST, Split.VALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m     76\u001b[39m         ]\n",
      "\u001b[31mKeyError\u001b[39m: 'test'"
     ]
    }
   ],
   "source": [
    "train_indices = np.random.choice(range(len(dataset['train'])), train_size, replace=False)\n",
    "test_indices = np.random.choice(range(len(dataset['test'])), test_size, replace=False)\n",
    "\n",
    "train_subset = [dataset['train'][i] for i in train_indices]\n",
    "test_subset = [dataset['test'][i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1fc55-fc8a-40a4-aa73-60bc036b3da9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca2f32-9900-456b-8471-4e0a40471b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = process_dataset(train_subset)\n",
    "X_test, y_test = process_dataset(test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c27e74-9a79-461b-9f1c-3808b938dbdd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d727b5-1fd2-4ce8-a2fe-cfc8faeddd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_indices = [label_mapping[label] for label in y_train]\n",
    "y_test_indices = [label_mapping[label] for label in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe84fb3-776e-4a8f-9fb4-75a85ad14852",
   "metadata": {},
   "source": [
    "### 3. Create PyTorch dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ebc774-71c4-458c-8c00-23eec2640fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab730888-6532-4ec7-9c1b-1f2f2dba161d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664278f-f7eb-4c72-aa93-150ce3dc66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214e252-d7e4-40d3-baac-ba247920ca1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e746629-34c3-4b45-86e9-201d107e55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(X_train_scaled, y_train_indices)\n",
    "test_dataset = MusicDataset(X_test_scaled, y_test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42239b78-c449-41f6-a304-a79c82d3dad9",
   "metadata": {},
   "source": [
    "### 4. Define the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d64bdf-35f0-4824-8a32-e9ac5dc9e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MusicGenreClassifier, self).__init__()\n",
    "        \n",
    "        # Architecture with dropout for regularization\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae846ca-c467-4aac-8a05-ecf22f4f4569",
   "metadata": {},
   "source": [
    "### 5. Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83245ba8-8403-41d3-a14f-4394ed257d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be44899-ce7d-4562-8d84-56e2f0673683",
   "metadata": {},
   "source": [
    "### 6. Evaluate model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c5d09-9a5b-42f2-b6f2-45e2a6699cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    # Initialize evaluate metric\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics using evaluate\n",
    "    accuracy = accuracy_metric.compute(predictions=all_preds, references=all_labels)\n",
    "    f1 = f1_metric.compute(predictions=all_preds, references=all_labels, average='macro')\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    genre_names = [inv_label_mapping[i] for i in range(len(inv_label_mapping))]\n",
    "    print(classification_report(all_labels, all_preds, target_names=genre_names))\n",
    "    \n",
    "    return accuracy, f1, cm, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1a92f-529f-4ac2-a8e4-340e1e1143cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Split training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1836c-3400-4938-9801-67adbb092e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_scaled, y_train_indices, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8463da6-55e4-4af3-a42f-ae352b0854d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create datasets for final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165e956-9c40-40cb-8744-d7cff89f0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_dataset = MusicDataset(X_train_final, y_train_final)\n",
    "val_dataset = MusicDataset(X_val, y_val)\n",
    "\n",
    "train_final_loader = DataLoader(train_final_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8534618-87d0-4df2-8d7d-e039c3dffe6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Initialize model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72779eaa-9867-4037-9002-bc50cf767a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # Number of features\n",
    "num_classes = len(label_mapping)\n",
    "model = MusicGenreClassifier(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b54a78-f530-4ac9-aa54-d8dec0a854e1",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d70ab-2931-4972-9398-ae7edc1d1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining the model...\")\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, criterion, optimizer, train_final_loader, val_loader, num_epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0de4b-cdfb-4642-9a3d-33da4d14d378",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0be81-594f-49b2-bf40-061fadbaa44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating the model...\")\n",
    "accuracy, f1, cm, all_preds, all_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy['accuracy']:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4f058-52e6-44f7-b5b1-b4f73d6e56ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfd846-20b0-47d2-9480-f5131c05c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[inv_label_mapping[i] for i in range(num_classes)],\n",
    "            yticklabels=[inv_label_mapping[i] for i in range(num_classes)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8aef97-9cd6-41a9-ac43-012d82b73df6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot training/validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e17b1-1475-440d-9dac-b900a99fd5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7d97c-21b4-4759-b58e-0aab3c69319c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f85c24-6902-4079-b276-61779dbeddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0ef70-beb3-4606-9824-cd7e27a6bced",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1349df3-b6e2-4f47-ac3b-4d635eb95bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'music_genre_classifier.pth')\n",
    "print(\"\\nModel saved as 'music_genre_classifier.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded5673-c481-46a3-87d9-8baa72debdf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example of using sklearn Pipeline with PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1aa4f-6e5f-44fb-a930-86233ed919fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchWrapper(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, model, criterion, optimizer, epochs=15, batch_size=32):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Convert data to PyTorch datasets\n",
    "        dataset = MusicDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{self.epochs}, Loss: {running_loss/len(dataset):.4f}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Convert to PyTorch tensor\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b6fdf-1147-4938-a029-585ab74dbb10",
   "metadata": {},
   "source": [
    "### Example of how to use the sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c792dd-abc8-4fb0-98fe-748929aac253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample of using sklearn Pipeline with PyTorch model:\")\n",
    "model_reset = MusicGenreClassifier(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_reset.parameters(), lr=0.001)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', PyTorchWrapper(model_reset, criterion, optimizer, epochs=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34db4a-6b15-4442-b262-74926e416ca5",
   "metadata": {},
   "source": [
    "### Note: This is just a demonstration - would typically use this on original data\n",
    "For brevity, we're using already processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e439ff-d36f-4870-880c-cef460711ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting pipeline...\")\n",
    "pipeline.fit(X_train, y_train_indices)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "accuracy = accuracy_metric.compute(predictions=y_pred, references=y_test_indices)\n",
    "print(f\"Pipeline Test Accuracy: {accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953d922-c1b3-4b4e-93ce-4b0e6be1f0b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Further improvements you could make:\n",
    "\n",
    "- Implement k-fold cross-validation\n",
    "- Try different neural network architectures\n",
    "- Implement early stopping\n",
    "- Explore feature importance\n",
    "- Add data augmentation techniques\n",
    "- Implement transfer learning using pretrained audio models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f5008-404b-450e-9cbe-72857a57b85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
