{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df844e4-3ebb-4c01-8158-fb9fee2a79a1",
   "metadata": {},
   "source": [
    "# Text Classification Project\n",
    "## Overview\n",
    "\n",
    "The project builds a sentiment classifier for movie reviews using both traditional machine learning (Logistic Regression) and deep learning (Neural Network) approaches. It uses the Rotten Tomatoes dataset to predict whether reviews are positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44c20c-6dd0-4b1d-b921-2653a6ce93f5",
   "metadata": {},
   "source": [
    "### 1. Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcb805-4e3a-4cd2-a6e7-dff9528d264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # For building neural networks\n",
    "import pandas as pd # For data manipulation and analysis\n",
    "import numpy as np # For numerical computations\n",
    "import scipy.sparse as sp # For efficient storage of sparse matrices\n",
    "\n",
    "# sklearn modules: For ML algorithms, text processing, and evaluation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import datasets # To easily load standard datasets\n",
    "import evaluate # For model evaluation metrics\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d02a73-44d3-4138-9cb5-eecc81cb31db",
   "metadata": {},
   "source": [
    "### 2. Load dataset using the datasets library\n",
    "\n",
    "This loads the Rotten Tomatoes dataset using the Hugging Face `datasets` library. This dataset contains movie reviews labeled as positive (1) or negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43dd2a6-e414-419b-a69f-1f12d836d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee180e-4210-4a6a-b34f-9c7138775e37",
   "metadata": {},
   "source": [
    "### 3. Convert to pandas and prepare data\n",
    "\n",
    "The code:\n",
    "- Converts the dataset splits into pandas DataFrames\n",
    "- Extracts the review text as features (x) and sentiment labels (y)\n",
    "- Creates separate lists for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104d9af7-2c8f-440a-8947-0a28534dd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879ba517-ddea-4c2b-b2c6-f8f48d4e2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['text'].tolist()\n",
    "y_train = train_df['label'].tolist()\n",
    "x_test = test_df['text'].tolist()\n",
    "y_test = test_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ca839-1e4c-4640-b6a4-2d2055811101",
   "metadata": {},
   "source": [
    "### 4. Feature extraction using sklearn and scipy\n",
    "\n",
    "This step:\n",
    "- Creates a TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer that limits to the 5,000 most important features\n",
    "- Transforms the raw text reviews into numerical feature vectors\n",
    "- TF-IDF represents words by their frequency in a document scaled by how rare they are across all documents\n",
    "- The result is a sparse matrix (mostly zeros) handled by `scipy.sparse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d34edd-50bd-4396-940a-132b1e12840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da20be3-dbc2-4b17-b4ba-054194ad087a",
   "metadata": {},
   "source": [
    "### 5. Train a traditional ML model using sklearn and scipy\n",
    "\n",
    "This section:\n",
    "- Creates a logistic regression model with 1,000 maximum iterations\n",
    "- Fits it to the training data\n",
    "- Generates predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c57a40-b3d2-427a-bb92-cad114d31fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(x_train_tfidf, y_train)\n",
    "lr_predictions = lr_model.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab06ab-36ea-402f-916a-d22a4957ae69",
   "metadata": {},
   "source": [
    "### 6. Neural Network with PyTorch\n",
    "#### Convert sparse matrices to dense tensors\n",
    "This section:\n",
    "- Converts the scipy sparse matrices to dense PyTorch tensors\n",
    "- Creates TensorDatasets that pair features with labels\n",
    "- Sets up DataLoaders that will feed data to the model in batches of 64 samples\n",
    "- Shuffles the training data to prevent learning order-specific patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9c7e325-1f15-4c24-b607-fba218980426",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.FloatTensor(x_train_tfidf.todense())\n",
    "x_test_tensor = torch.FloatTensor(x_test_tfidf.todense())\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9fab37-7269-4140-9879-781141f92aa8",
   "metadata": {},
   "source": [
    "Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cd6ec5-9182-4a6d-8adc-4c84cc4af2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f17e5-7f63-4a3b-892e-abb474ace9f5",
   "metadata": {},
   "source": [
    "Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0184615e-4259-4fc0-9533-94e74894bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2) #Binary classification\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_dim = x_train_tfidf.shape[1]\n",
    "model = TextClassifier(input_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3975603-779e-41bd-b841-9c841c357409",
   "metadata": {},
   "source": [
    "Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "006f4a10-728f-4e56-bff6-949e0f5a0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d557a28-a25f-434a-8ef7-87ff95447b9e",
   "metadata": {},
   "source": [
    "Evaluate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f0e6945-7046-482d-9a31-fe2d13b0e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "nn_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        nn_predictions.extend(predicted.cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e60c4-5fae-4054-8140-282c759572ba",
   "metadata": {},
   "source": [
    "### 7. Evaluate both models using evaluate and sklearn\n",
    "\n",
    "This section:\n",
    "\n",
    "- Uses the `evaluate` library to calculate accuracy\n",
    "- Uses `sklearn.metrics` to calculate precision, recall and F1-score\n",
    "- Evaluates both models with the same metrics for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73436f57-5ad2-42b0-a1d0-78aad97ddf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4f76f8289f45d49e896480946699ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "lr_accuracy = metric.compute(predictions=lr_predictions, references=y_test)\n",
    "nn_accuracy = metric.compute(predictions=nn_predictions, references=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b8f81-4c3f-42c5-ad50-2610c1b8b0cd",
   "metadata": {},
   "source": [
    "Calculate precision, recall, and F1 using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e00576-52a3-4b12-ac65-e1031c1213a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_precision, lr_recall, lr_f1, _ = precision_recall_fscore_support(y_test, lr_predictions, average='binary')\n",
    "nn_precision, nn_recall, nn_f1, _ = precision_recall_fscore_support(y_test, nn_predictions, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f66706-27d2-4494-adfe-1d90470219ff",
   "metadata": {},
   "source": [
    "### 8. Compare results\n",
    "\n",
    "This creates and displays a pandas DataFrame that directly compares the performance of both models across multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58ba446-e692-4a6a-afd4-72b097ec290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.770169   0.775862  0.759850  0.767773\n",
      "1       Neural Network  0.767355   0.783300  0.739212  0.760618\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression',  'Neural Network'],\n",
    "    'Accuracy': [lr_accuracy['accuracy'], nn_accuracy['accuracy']],\n",
    "    'Precision': [lr_precision, nn_precision],\n",
    "    'Recall': [lr_recall, nn_recall],\n",
    "    'F1 Score': [lr_f1, nn_f1],\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14e82a-78f9-45d6-b6d4-894071625e0f",
   "metadata": {},
   "source": [
    "### 9. Feature importance analysis with scipy and numpy\n",
    "\n",
    "This final section:\n",
    "- Extracts coefficient values from the logistic regression model\n",
    "- Uses numpy to calculate absolute importance\n",
    "- Pairs them with the feature names from the TF-IDF vectorizer\n",
    "- Creates a DataFrame of feature importance\n",
    "- Displays the top 10 most important words for sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0af34aa9-7c7b-4c13-b6f0-d16e125a9c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features: \n",
      "           Feature  Importance\n",
      "4528           too    4.231535\n",
      "351            bad    3.068830\n",
      "1327          dull    2.903202\n",
      "3214  performances    2.587575\n",
      "224            and    2.538227\n",
      "4196         still    2.458881\n",
      "500         boring    2.444609\n",
      "3084          only    2.376841\n",
      "4948         worst    2.213899\n",
      "1452     enjoyable    2.194255\n"
     ]
    }
   ],
   "source": [
    "if hasattr(lr_model, 'coef_'):\n",
    "    # Get feature importance from logistic regression\n",
    "    importance = np.abs(lr_model.coef_[0])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    })\n",
    "\n",
    "    # Display top 10 most important features\n",
    "    top_features = feature_importance.sort_values('Importance', ascending=False).head(10)\n",
    "    print(\"\\nTop 10 Most Important Features: \")\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab8bac-e6eb-4280-91d6-82ef639a91cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
