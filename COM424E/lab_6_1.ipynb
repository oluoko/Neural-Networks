{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6e3299-5ab5-4e00-8af0-16a60a483c6a",
   "metadata": {},
   "source": [
    "# Calculating Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec758b23-5f1e-4da7-b7e4-aa9ca98f2d9f",
   "metadata": {},
   "source": [
    "### With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the values from the softmax distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0046b0e0-0f0e-443c-bbcb-b13598b60a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = [[0.7,0.1,0.2],\n",
    "                   [0.1,0.5,0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "class_targets = [0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a65ad9-a7eb-43f3-a25c-fdb52145bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f1445-7168-43d3-aff1-8e342d994ac4",
   "metadata": {},
   "source": [
    "#### This can be further simplified using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c455a2-79e9-403d-b9e1-058eb4de1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb56f11-1cca-41c1-a0ce-bede96136072",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = np.array([[0.7,0.1,0.2],\n",
    "                   [0.1,0.5,0.4],\n",
    "                   [0.02, 0.9, 0.08]])\n",
    "class_targets = [0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759f5c67-ee46-4106-a07d-1170b31b0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[[0,1,2],class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc65126-6ce0-4251-ba4e-44fa4b9cad40",
   "metadata": {},
   "source": [
    "#### We know weâ€™re going to have as many indices as distributions in our entire batch, so we can use a ` range() ` instead of typing each value ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d0bba6-4640-4d8f-bfed-74c7eaa701b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb40176-e647-47ed-aa46-b6cfd3cb7909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(pred_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef97db1-f7bb-4bc8-9732-510b4613c223",
   "metadata": {},
   "source": [
    "##### This returns a list of the confidences at the target indices for each of the samples. Now we apply the negative log to this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8bf47e-1eee-4842-b760-7b1c0f16b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(pred_confidences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8186515-520e-42ba-98e8-109482c7470d",
   "metadata": {},
   "source": [
    "##### NumPy has a method that computes this average on arrays, so we will use that to have an idea about how our model is doing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ca95940-5b96-4699-b781-a4475ee5031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "average_loss =np.mean(-np.log(pred_confidences))\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392e3b1-10bc-42ad-bd04-376238987862",
   "metadata": {},
   "source": [
    "#### We have to add a test to the code we just wrote for the number of dimensions, move calculations of the log values outside of this new if statement, and implement the solution for the one-hot encoded labels following the first equation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d42a8d-d0c1-421b-b7e1-b8fa42a65aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_targets = np.array([[1,0,0],\n",
    "                          [0,1,0],\n",
    "                          [0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf3bbd9-1973-46c8-b8ef-af76413fc975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class targets shape:  2\n"
     ]
    }
   ],
   "source": [
    "# Probabilities for target values\n",
    "# only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    print(\"Class targets shape: \", len(class_targets.shape))\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "    \n",
    "\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    print(\"Class targets shape: \", len(class_targets.shape))\n",
    "    corrent_confidences = np.sum(\n",
    "        softmax_outputs*class_targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc9b9ea-d7c2-4e9d-b4d8-0da8797f6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "neg_log = -np.log(corrent_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ba7968f-e955-4d11-8238-b791670d9568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944bd62-5c64-4d76-b1b4-2c5356a0166b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
